{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>TP Régression Logistique</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> SD211 </h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je vous prie de bien vouloir vérifier que les fichiers .py .ipynb et les données sont bien dans le même dossier avant execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from diabeticRetinopathyUtils import load_diabetic_retinopathy\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Régularisation de Tikhonov "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a :\n",
    "$$ f_{1} (w_{0},w) =\\dfrac {1} {n}\\sum _{i=1}^{n}\\log \\left( 1+e^{-y_{i}\\left( {x_{i}}^{T}w+w_{0}\\right) }\\right) +\\dfrac {\\rho } {2}\\left\\| w\\right\\| _{2}^{2} $$\n",
    "\n",
    "avec: \n",
    "\n",
    "$  ~~~~~~~~ X=\\left( \\begin{matrix} x_{1}^{T}\\\\ \\vdots \\\\ x_{n}^{T}\\end{matrix} \\right) \\in \\mathbb{R} ^{n\\times p } ~,~~~~ W=\\left( \\begin{matrix} w_{1}\\\\ \\vdots \\\\ w_{p}\\end{matrix} \\right) \\in \\mathbb{R} ^{ p \\times 1 } ~,~~~~ Y=\\left( \\begin{matrix} y_{1}\\\\ \\vdots \\\\ y_{n}\\end{matrix} \\right) \\in \\mathbb{R} ^{n \\times 1 } ~,~~~~ w_{0} \\in \\mathbb{R} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Gradient de $f_1$:**\n",
    "\n",
    "$$ \\nabla_{w_{0}}f_{1} = \\frac{\\partial f_{1}}{\\partial w_{0}}=\\frac{-1}{n}\\sum_{i=1}^{n}{\\frac{y_{i}}{1+exp(y_{i}(x_{i}^{T}w+w_{0}))}} \n",
    "$$\n",
    "\n",
    "$$ \\nabla_{w}f_{1}= \\frac{\\partial f_{1}}{\\partial w}=\\frac{-1}{n}\\sum_{i=1}^{n}{\\frac{y_{i}}{1+exp(y_{i}(x_{i}^{T}w+w_{0}))}x_{i}}+\\rho w $$\n",
    "\n",
    "\n",
    "donc: \n",
    "\n",
    "$$ \\fbox{$\\nabla f_{1}= \\begin{pmatrix} \n",
    "\\nabla_{w_{0}}f_{1} \\\\ \n",
    "\\nabla_{w}f_{1} \n",
    " \\end{pmatrix} =  \\begin{pmatrix} \n",
    "\\frac{-1}{n}\\sum_{i=1}^{n}{\\frac{y_{i}}{1+exp(y_{i}(x_{i}^{T}w+w_{0}))}}  \\\\ \n",
    "\\frac{-1}{n}\\sum_{i=1}^{n}{\\frac{y_{i}}{1+exp(y_{i}(x_{i}^{T}w+w_{0}))}x_{i}}+\\rho w \n",
    " \\end{pmatrix} $}\\in \\mathbb{R} ^{\\left( p+1\\right) \\times 1 }$$\n",
    "\n",
    "\n",
    "\n",
    "* **Hessienne de $f_1$:**\n",
    "\n",
    "Si on ajoute une colonne de \"1\" au début de la matrice X à l'indice 0, on obtient la matrice $\\tilde{X} = \\left( \\begin{matrix} 1& x_{1}^{T}\\\\ 1& x_{2}^{T}\\\\ \\vdots & \\vdots \\\\ 1& x_{n}^{T}\\end{matrix} \\right)= \\left( \\begin{matrix} \\tilde{x_{1}^{T}}\\\\ \\vdots \\\\ \\tilde{x_{n}^{T}}\\end{matrix} \\right) \\in \\mathbb{R} ^{n\\times \\left( p+1\\right) }$ <br>\n",
    "\n",
    "et si on désigne par   $\\tilde{w} = \\left( \\begin{matrix} w_0\\\\ w\\end{matrix} \\right) \\in \\mathbb{R} ^{ (p+1) \\times 1 } $ ,en calculant la matrice hessienne, on obtient donc:\n",
    "\n",
    "Donc: \n",
    "$$\\fbox{$H =  \\dfrac {1} {n}\\sum _{i=1}^{n}y_{i}^2 \\dfrac{ {e}^{+ y_i\\tilde{x_i}^{T} \\tilde{w}} } { (1+{e}^{+ y_i\\tilde{x_i}^{T} \\tilde{w}})^2} \\tilde{x_{i}} \\tilde{x_{i}}^T + \\rho (I_{p+1} - \\left( \\begin{matrix} 1& 0& \\ldots & & 0\\\\ 0& 0& \\ldots & & 0\\\\ & & \\ddots & & \\vdots \\\\ 0& 0& ..& .& 0\\end{matrix} \\right))$}  \\in \\mathbb{R} ^{\\left( p+1\\right) \\times \\left( p+1\\right) }$$\n",
    "\n",
    "Avec: \n",
    "\n",
    "$$\\tilde{x_{i}} \\tilde{x_{i}}^T= \\left(\\begin{matrix} 1& x_{i,1}& \\ldots & & x_{i,p}\\\\ x_{i,1}& x_{i,1}^2& \\ldots & & .\\\\ & & \\ddots & & \\vdots \\\\ x_{i,p}& .& ..& .& x_{i,p}^2\\end{matrix}\\right) ~~~  \\in \\mathbb{R} ^{(p+1)\\times \\left( p+1\\right) }  ~,~~~~  \\forall i \\in\\left[0,n\\right]  $$  \n",
    "\n",
    "et: \n",
    "\n",
    "$$I_{p+1} = \\left( \\begin{matrix} 1& 0& \\ldots & & 0\\\\ 0& 1& \\ldots & & 0\\\\ & & \\ddots & & \\vdots \\\\ 0& 0& ..& .& 1\\end{matrix} \\right) ~~~  \\in \\mathbb{R} ^{(p+1)\\times \\left( p+1\\right) } $$\n",
    "\n",
    "\n",
    "* **Convexité de $f_1$:**\n",
    "\n",
    "Pour montrer que <b>$f_1$ </b> est <b>convexe</b>, il suffirait de montrer que la matrice hessienne <b>H</b> correspondante est <b>définie positive: </b><br><br>\n",
    "Soit $ v\\in \\mathbb{R} ^{ p+1 }$,$v \\neq 0^{ ~ p+1 }$, calculons $v^{T}Hv$ : <br>\n",
    "$v^{T}Hv = \\dfrac {1} {n}\\sum _{i=1}^{n} \\dfrac{ {e}^{+ y_i\\tilde{x_i}^{T} \\tilde{w}} } { (1+{e}^{+ y_i\\tilde{x_i}^{T} \\tilde{w}})^2} v^T\\tilde{x_{i}} \\tilde{x_{i}}^T v + \\rho v^T(I_{p+1} - \\left( \\begin{matrix} 1& 0& \\ldots & & 0\\\\ 0& 0& \\ldots & & 0\\\\ & & \\ddots & & \\vdots \\\\ 0& 0& ..& .& 0\\end{matrix} \\right))v $ <br>\n",
    "$v^{T}Hv = \\dfrac {1} {n}\\sum _{i=1}^{n} \\beta_i (\\tilde{x_{i}}^Tv)^2 + \\rho \\sum _{j=1}^{p}v_{j}^{2} > 0$ &nbsp; &nbsp; &nbsp; où &nbsp; $\\beta_i = \\dfrac{ {e}^{+ y_i\\tilde{x_i}^{T} \\tilde{w}} } { (1+{e}^{+ y_i\\tilde{x_i}^{T} \\tilde{w}})^2} > 0$ <br>\n",
    "Donc H est définie positive, \n",
    "\n",
    "d'où <b>$f_1$ </b> est <b>convexe</b>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "filename = 'diabeticRetinopathy.csv'\n",
    "X,y = load_diabetic_retinopathy(filename);\n",
    "# saving the data format\n",
    "n = np.shape(X)[0]\n",
    "p = np.shape(X)[1]\n",
    "# Adding a column of ones to X to simplify calculations \n",
    "X_tilde = np.concatenate( (np.ones((n,1)),X), axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating value, gradient and hessian of f1\n",
    "\n",
    "# Required parameters that aren't defined inside the function\n",
    "X, X_tilde, y, n, p\n",
    "rho = 1/n\n",
    "\n",
    "def calculatef1(w_0,w):\n",
    "    w_tilde = np.concatenate(([w_0],w), axis=0) #create a vector of w_0 and w \n",
    "    temp = np.exp( -y*X_tilde.dot(w_tilde) ) #frequent expression \n",
    "    \n",
    "    #value of f1\n",
    "    v1 = rho * ((np.linalg.norm(w))**2) / 2\n",
    "    v2 = np.mean( np.log( 1 + temp ) )\n",
    "    f1 = v1 + v2\n",
    "    \n",
    "    #grad of f1\n",
    "    expy = np.exp( y*X_tilde.dot(w_tilde) )\n",
    "    gradf1 = np.zeros((p+1,1))\n",
    "    gradf1[0] = np.mean( -y*temp/(1+temp) )\n",
    "    for j in range(1,p+1):\n",
    "        gradf1[j] = np.mean( -y*X[:,j-1]*temp / (1+temp) ) + rho*w[j-1]\n",
    "    \n",
    "    #Hessian of f1\n",
    "    v = np.exp( y*(X_tilde.dot(w_tilde)))\n",
    "    # We need the reshape cause X_tilde[i,:]'s shape is (20,) multiplying with X_tilde[i,:].T will output a scalar not a matrix as expected\n",
    "    H_ = 1/n * sum( v[i] / ((1+ v[i])**2) * ( np.reshape(X_tilde[i,:],(p+1,1)).dot(np.reshape(X_tilde[i,:],(1,p+1))) ) for i in range(n) )  \n",
    "    A = np.identity(p+1)\n",
    "    A[0,0] = 0\n",
    "    H = H_ + rho * A\n",
    "    \n",
    "    return f1, gradf1, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checking the function's output\n",
    "#f,gra,H = calculatef1(1,np.ones((19,)))\n",
    "#print ('Calculating f1 for w = \\n %s \\n' % np.ones((p+1,)))\n",
    "#print ('The value of the function f1 is \\n %f \\n' % f)\n",
    "#print ('The calculated gradient is : \\n %s \\n' % gra )\n",
    "#print ('The calculated Hessian is : \\n %s \\n' % H )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification du gradient par la fonction check_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolute error is : 3.669272e-08 \n",
      "relative error is : 1.116008e-07 \n"
     ]
    }
   ],
   "source": [
    "# locally needed imports\n",
    "from numpy import ravel\n",
    "from scipy.optimize import check_grad\n",
    "\n",
    "def func_f1(w_tilde):\n",
    "    return calculatef1(w_tilde[0], w_tilde[1:])[0]\n",
    "\n",
    "def func_gradf1(w_tilde):\n",
    "    return calculatef1(w_tilde[0], w_tilde[1:])[1].ravel()\n",
    "\n",
    "w_tilde = np.zeros((p+1,))\n",
    "\n",
    "error = check_grad(func_f1,func_gradf1,w_tilde)\n",
    "print ('absolute error is : %e ' % error)\n",
    "rel_error = error / np.linalg.norm(func_gradf1(w_tilde))\n",
    "print ('relative error is : %e ' % rel_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La valeur de l'erreur étant non significative, on pourra affirmer que l'implémentation et le calcul du gradient sont corrects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needed parameters that are not defined inside the function\n",
    "X, X_tilde, y, n, p, rho\n",
    "\n",
    "# simple Newton's method\n",
    "def newton(f,w_tilde0):\n",
    "    t = time()\n",
    "    itera = 0\n",
    "    g_norms = []\n",
    "    w_tilde = w_tilde0\n",
    "    while True:\n",
    "        itera += 1\n",
    "        try :\n",
    "            _,g,h = f( np.reshape(w_tilde[0],()), np.reshape(w_tilde[1:],(p,))) \n",
    "            w_tilde = np.reshape(w_tilde,(20,1)) - np.linalg.inv(h).dot(g)\n",
    "            g_norm = np.linalg.norm(g)\n",
    "            if math.isnan(g_norm): #if the value reached is nan\n",
    "                print('Overflow !')\n",
    "                return None, None\n",
    "        except OverflowError: #if the values overflow\n",
    "            print(\"Overflow !\")\n",
    "            return None, None\n",
    "        g_norms.append(g_norm) #saving the gradient's norms to plot them later\n",
    "        if (g_norm < 10**(-10)):\n",
    "            print(\"Newton's method terminated in %d iterations\" % itera)\n",
    "            print(\"Newton's method terminated in %f ms\" % (1000*(time()-t)) ) \n",
    "            break;\n",
    "    return w_tilde, g_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton's method terminated in 9 iterations\n",
      "Newton's method terminated in 116.150379 ms\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fba9c0b6cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w_tilde0 = np.zeros((p+1,))\n",
    "# applying newton's method starting from w_tilde0\n",
    "w_tilde_star, g_norms = newton(calculatef1,w_tilde0)\n",
    "\n",
    "plt.plot(g_norms,'bo-')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Decrease of the norm of the gradient in Newton's method\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"norm of the gradient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\Rightarrow $ On note que la norme du gradient décroit au fur et à mesure qu'on itère.\n",
    " \n",
    " Pour cette condition initiale $(w_0⁰,w⁰)=0$, l'algorithme de Newton <b>converge</b> vers la solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67731198,  0.28526576, -0.24554742,  5.61365006,  0.9888679 ,\n",
       "        -1.82791918, -2.55695256, -1.58747255,  0.43396361,  0.48515529,\n",
       "        -0.40441764,  0.17280405, -0.57564945,  0.43832626, -0.62435831,\n",
       "         1.25356177,  0.72621094, -0.00607139, -0.1490878 , -0.09505212]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieved solution for (w0,w)_init = 0\n",
    "np.reshape( w_tilde_star , (1,20) ) #reshaping just for display purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overflow !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeghampc/Softwares/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/jeghampc/Softwares/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  if __name__ == '__main__':\n",
      "/home/jeghampc/Softwares/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:17: RuntimeWarning: overflow encountered in exp\n",
      "/home/jeghampc/Softwares/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/jeghampc/Softwares/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/jeghampc/Softwares/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning: overflow encountered in multiply\n",
      "/home/jeghampc/Softwares/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:24: RuntimeWarning: overflow encountered in exp\n",
      "/home/jeghampc/Softwares/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "w_tilde03 = 0.3*np.ones((p+1,))\n",
    "# applying newton's method starting from w_tilde03\n",
    "w_tilde_03, g_norms03 = newton(calculatef1,w_tilde03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow $ En changeant le point d'initialisation de <b> w_tilde0 </b> à <b> w_tilde03 </b>, l'algorithme ne converge plus. \n",
    "En effet, le code retourne une exception vu qu'au cours des itérations, le gradient prend de grandes valeurs.\n",
    "L'algorithme de Newton à pas constant ne donne pas une solution que si le point de départ est suffisamment proche de la solution (c'est à dire le point où le min est atteint).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De façon générale,  l'algorithme de Newton dépend du choix de l’initialisation $w_{init}$ et des propriétés de la fonction qui doit être 3-différentiable. \n",
    "\n",
    "Donc une façon pour remédier à ce problème est d'utiliser la recherche linéaire (<b> Armijo's Linear Search Method ) </b> dont le principe est le suivant: \n",
    "\n",
    "Etant donnés $a$ ∈ (0, 1),$b$ > 0 et $β$ ∈ (0, 1), on veut determiner le meilleur entier $l$ vérifiant l'inégalité suivante: \n",
    "\n",
    "$$\\quad f(x^+ (ba^l)) ≤ f(x_{k}) + β <∇f(x_{k}), x^+ ba^l − x_{k}> $$\n",
    "\n",
    "avec: $\\gamma_{k} =  ba^l $\n",
    "\n",
    "et $ \\quad x^+ (\\gamma_{k}) = x^+ (ba^l) = x_{k} - \\gamma_{k} \\nabla f (x_{k})$ \n",
    "\n",
    "ici on prend $f = f_{1} $et $x= $w_tilde , on aura alors: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Remarque : </h4>\n",
    "le choix des paramètres a été effectués en choisissant plusieurs valeurs des paramètres. Finalement, on a choisi $b=1$, $a=0.5$ et $\\beta=0.5$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needed parameters that are not defined inside the function\n",
    "X, X_tilde, y, n, p, rho\n",
    "\n",
    "#Newton's method with linear search\n",
    "def linearNewton(f,w_tilde0,a=0.5,b=1,beta=0.5):\n",
    "    itera = 0\n",
    "    t = time()\n",
    "    g_norms = []\n",
    "    w_tilde = w_tilde0\n",
    "    while True:\n",
    "        itera += 1\n",
    "        print(\",\", end='') #just printing to make sure the algorithm is still running\n",
    "        f_k,g,h = f( np.reshape(w_tilde[0],()), np.reshape(w_tilde[1:],(p,))) \n",
    "        l=0\n",
    "        while True:\n",
    "            print(\".\", end='') #just printing to make sure the algorithm is still running\n",
    "            w_plus = np.reshape(w_tilde,(p+1,1)) - b*(a**l)*np.linalg.inv(h).dot(g)\n",
    "            f_plus = f( np.reshape(w_plus[0],()), np.reshape(w_plus[1:],(p,)))[0]\n",
    "            term_d = f_k + beta * np.reshape( np.transpose(g).dot( np.reshape(w_plus,(20,1))-np.reshape(w_tilde,(20,1)) ) ,())\n",
    "            if f_plus > term_d:\n",
    "                l = l+1\n",
    "            else:\n",
    "                break;\n",
    "        w_tilde = np.reshape(w_tilde,(p+1,1)) - b*(a**l)*np.linalg.inv(h).dot(g)\n",
    "        g_norm = np.linalg.norm(g)\n",
    "        if math.isnan(g_norm):\n",
    "            print()\n",
    "            print('Overflow !')\n",
    "            return None, None\n",
    "        g_norms.append(g_norm)\n",
    "        if (g_norm < 10**(-10)):\n",
    "            print()\n",
    "            print(\"Newton's method with linear search terminated in %d iterations\" % itera)\n",
    "            print(\"Newton's method with linear search terminated in %f ms\" % (1000*(time()-t)))\n",
    "            break;\n",
    "    return w_tilde, g_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",..,.,.,.,.,.,..,..,.,.\n",
      "Newton's method with linear search terminated in 10 iterations\n",
      "Newton's method with linear search terminated in 348.394156 ms\n"
     ]
    }
   ],
   "source": [
    "# Starting from the same previous starting point w_tilde03 but using linear search\n",
    "w_tilde03_star, nor03 = linearNewton(calculatef1,w_tilde03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.67731198  0.28526576 -0.24554742  5.61365006  0.9888679  -1.82791918\n",
      "  -2.55695256 -1.58747255  0.43396361  0.48515529 -0.40441764  0.17280405\n",
      "  -0.57564945  0.43832626 -0.62435831  1.25356177  0.72621094 -0.00607139\n",
      "  -0.1490878  -0.09505212]]\n"
     ]
    }
   ],
   "source": [
    "# Retrieved solution from w_tilde03 and with newton's method with linear search\n",
    "print( np.reshape(w_tilde03_star, (1,20))) #reshaping just for display purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Régularisation par la parcimonie "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a:  $$F_2 = \\dfrac {1} {n}\\sum _{i=1}^{n}\\log \\left( 1+e^{-y_{i}\\left( {x_{i}}^{T}w+w_{0}\\right) }\\right) + \\rho \\left\\| w\\right\\| _{1}$$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode de Newton suppose que la fonction est 3-différentiable . Or cette condition n'est pas vérifiée dans ce cas. <br> \n",
    "En effet, $w_{1}\\rightarrow \\rho \\left\\| w\\right\\| _{1}$ n'est même pas de classe $C^1$ sur $ \\mathbb{R} ^{p}$ (n'est pas strictement différentiable) \n",
    "\n",
    "donc la méthode de Newton n'est pas applicable dans ce cas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a:  $$F_2 = \\dfrac {1} {n}\\sum _{i=1}^{n}\\log \\left( 1+e^{-y_{i}\\left( {x_{i}}^{T}w+w_{0}\\right) }\\right) + \\rho \\left\\| w\\right\\| _{1}$$ <br>\n",
    "* **Décomposition de $F_2$:** \n",
    "\n",
    "on décompse $F_2$ sous la forme $F_2 = f_2 + g_2$ où $$\\boxed{f_2 = \\dfrac {1} {n}\\sum _{i=1}^{n}\\log \\left( 1+e^{-y_{i}\\left( {x_{i}}^{T}w+w_{0}\\right) }\\right)}$$  et $$\\boxed{g_2 = \\rho \\left\\| w\\right\\| _{1}}$$\n",
    "\n",
    "* **Formule de l'opérateur Proximal de $g_2$:** \n",
    "\n",
    "On a: \n",
    "$g(w) = \\rho \\sum _{i=1}^{p}\\left| w_{i}\\right|$\n",
    "\n",
    "la fonction $g_2(w)$ est séparable et son proximal a été calculée dans le cours et dans le TD;\n",
    "\n",
    "$$\n",
    "\\textrm{prox}_{g_{2}}:\\left(\\omega\\right)\\mapsto\\left(\\begin{array}{l}\n",
    "\\textrm{prox}_{g_{21}}\\left(\\omega\\right)\\\\\n",
    "\\textrm{prox}_{g_{22}}\\left(\\omega\\right)\\\\\n",
    "...\\\\\n",
    "\\textrm{prox}_{g_{2n}}\\left(\\omega\\right)\n",
    "\\end{array}\\right) , ~~~~~~~~~~~~avec ~~~~ \\omega=\\left(\\begin{array}{l}\n",
    "\\omega_{1}\\\\\n",
    "...\\\\\n",
    "\\omega_{n}\n",
    "\\end{array}\\right) \n",
    "$$ \n",
    "c'est à dire:\n",
    "$$ p = (prox_{g_2}(w))_i = \\arg\\min_{y \\in \\mathbb{R}}(\\rho \\left|y\\right| + \\frac{(y-x)^2}{2})$$ <br>\n",
    "\n",
    "\n",
    "donc pour tout $ i\\in\\{1,..,n\\}$ :\n",
    "$$   \n",
    "\\textrm{prox}_{g_{2i}}:\\left(\\omega\\right)\\mapsto\\begin{cases}\n",
    "\\omega_{i}-\\rho & \\omega_{i}\\geq\\rho\\\\\n",
    "0 & \\left|\\omega_{i}\\right|<\\rho\\\\\n",
    "\\omega_{i}+\\rho & \\omega_{i}\\leq-\\rho\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* **Gradient de $f_2$:** \n",
    "\n",
    "Le gradient de $f_2$ se déduit facilement de celui de $f_1$\n",
    "\n",
    "$$ \\fbox{$\\nabla f_{2}= \\begin{pmatrix} \n",
    "\\nabla_{w_{0}}f_{2} \\\\ \n",
    "\\nabla_{w}f_{2} \n",
    " \\end{pmatrix} =  \\begin{pmatrix} \n",
    "\\frac{-1}{n}\\sum_{i=1}^{n}{\\frac{y_{i}}{1+exp(y_{i}(x_{i}^{T}w+w_{0}))}}  \\\\ \n",
    "\\frac{-1}{n}\\sum_{i=1}^{n}{\\frac{y_{i}}{1+exp(y_{i}(x_{i}^{T}w+w_{0}))}x_{i}} \n",
    " \\end{pmatrix} $}\\in \\mathbb{R} ^{\\left( p+1\\right) \\times 1 }$$\n",
    "\n",
    "* **Convexité de $F_2$:** \n",
    "\n",
    "$F_2$ est convexe car $g_2$ et $f_2$ le sont. En effet, la convexité de $f_2$ se démontre de façon similaire à la partie 1 et $g_2$ est une multiplication de la norme-1 par $\\rho$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rho = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needed parameters that are not defined inside the function\n",
    "p, X_tilde, X, y, rho\n",
    "\n",
    "def calculatef2(w_tilde):\n",
    "    if ( len(w_tilde) != p+1 ):\n",
    "        print(\" __ calculatef2 __ Inappropriate w_tilde length\")\n",
    "    w_tilde = np.reshape(w_tilde, (p+1,1))\n",
    "    temp = np.exp( -y*X_tilde.dot(w_tilde) ) #frequent expression \n",
    "    \n",
    "    #value of f2\n",
    "    f2 = np.mean( np.log( 1 + temp ) )\n",
    "    \n",
    "    #grad of f2\n",
    "    expy = np.exp( y*X_tilde.dot(w_tilde) )\n",
    "    gradf2 = np.zeros((p+1,1))\n",
    "    gradf2[0] = np.mean( -y*temp/(1+temp) )\n",
    "    for j in range(1,p+1):\n",
    "        gradf2[j] = np.mean( -y*X[:,j-1]*temp / (1+temp) )\n",
    "    \n",
    "    return f2, gradf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateg2(w_tilde,rho=0.1):\n",
    "    if (len(w_tilde) != p+1 ):\n",
    "        print(\"__ calculateg2 __ Inappropriate w_tilde length \")\n",
    "    return rho*np.sum(np.abs( w_tilde[1:] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Proximal operator for g_2\n",
    "# w_tilde must be a np.array\n",
    "def prox_g2(w_tilde,alpha=1,rho = 0.1):\n",
    "    if ( type(w_tilde).__module__ != np.__name__ ):\n",
    "        print(\"__ prox_g2 __ Input parameter must be a numpy.array !\")\n",
    "        w_tilde = np.reshape(w_tilde, (len(w_tilde),1) )\n",
    "    if (len(w_tilde) != p+1 ):\n",
    "        print(\"__ prox_g2 __ Inappropriate w_tilde length \")\n",
    "    def compare(w):\n",
    "        if w > rho*alpha:\n",
    "            return w-rho*alpha\n",
    "        if w < -rho*alpha:\n",
    "            return w+rho*alpha\n",
    "        return 0\n",
    "    pro = np.reshape( list(map(compare, w_tilde)) , np.shape(w_tilde) )\n",
    "    print(np.shape(pro))\n",
    "    pro[0] = 0\n",
    "    return  pro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On test plusieurs test d'arrêt. mais celui qui sera effectivement utilisé est le test sur la variation de la solution retrouvés c'est à dire qu'entre deux itérations $w_{k+1}$ ne varie pas trop par rapport à $w_{k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rho = 0.1\n",
    "rho, p\n",
    "\n",
    "def ProximalGradient_LinearSearch( w_tilde , f2, g2, prox_g2, a=0.5, b=1, beta=0.5, rho = 0.1):\n",
    "    t = time()\n",
    "    gradf2_norms = []\n",
    "    w_tilde = np.reshape( w_tilde0, (p+1,1) )\n",
    "    \n",
    "    itera = 0\n",
    "    while True:\n",
    "        #for termination conditions\n",
    "        itera = itera +1 \n",
    "        wk = w_tilde \n",
    "        \n",
    "        f2_k,grad_f2 = f2( w_tilde ) \n",
    "        if np.shape(grad_f2) != (20, 1):\n",
    "            print(\"__ ProximalGradient_LinearSearch __ Error in shape of gradf2 !\")\n",
    "        \n",
    "        # line search for f2 to find gamma\n",
    "        l=0\n",
    "        while l<40:\n",
    "            if np.shape(w_tilde) != (20, 1):\n",
    "                print(\"__ ProximalGradient_LinearSearch __ Error in shape of w_tilde !\")\n",
    "            w_plus = w_tilde - b*(a**l) * grad_f2\n",
    "            if np.shape(w_plus) != (20, 1):\n",
    "                print(\"__ ProximalGradient_LinearSearch __ Error in shape of w_plus !\")\n",
    "            term_d = f2_k + beta * np.transpose(grad_f2).dot( w_plus - w_tilde )\n",
    "            if f2(w_plus)[0] > term_d:\n",
    "                l = l+1\n",
    "            else:\n",
    "                break;\n",
    "        \n",
    "        # Proximal gradient on g2\n",
    "        w_tilde = prox_g2(w_plus, alpha=b*(a**l), rho=rho)\n",
    "        \n",
    "        gradf2_norm = np.linalg.norm(f2( w_tilde )[1])\n",
    "        if np.shape(w_tilde) != (20, 1):\n",
    "                print(\"__ ProximalGradient_LinearSearch __ Error in shape of w_k+1 !\")\n",
    "        if math.isnan(gradf2_norm):\n",
    "            print('Overflow !')\n",
    "            return None, None\n",
    "        gradf2_norms.append(gradf2_norm)\n",
    "        print(\"current objective value: \")\n",
    "        print(f2(w_tilde)[0] + g2(w_tilde,rho))\n",
    "        print(\"norm of current gradient: \")\n",
    "        print(gradf2_norm)\n",
    "        \n",
    "        # Termination conditions\n",
    "        if (gradf2_norm < 10**(-10)):\n",
    "            print('__ Terminated __ Gradient Condition, after %d iteration' % itera)\n",
    "            print(\"Proximal gradient with linear search terminated in %f ms\" % (1000*(time()-t)))\n",
    "            break;\n",
    "        if (np.allclose(wk, w_tilde)):\n",
    "            print('__ Terminated __ w Condition, after %d iteration' % itera)\n",
    "            print(\"Proximal gradient with linear search terminated in %f ms\" % (1000*(time()-t)))\n",
    "            break;\n",
    "    return w_tilde, gradf2_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n",
      "current objective value: \n",
      "0.69314718056\n",
      "norm of current gradient: \n",
      "0.328785478355\n",
      "__ Terminated __ w Condition, after 1 iteration\n",
      "Proximal gradient with linear search terminated in 10598.962307 ms\n"
     ]
    }
   ],
   "source": [
    "w_star_prox,_ = ProximalGradient_LinearSearch(w_tilde03, calculatef2, calculateg2, prox_g2, beta=0.5, b=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Comparaison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le premier problème est un problème d'optimisation convexe avec des fonctions trois différentiables ce qui nous a offert l'opportunité d'utiliser la méthode de newton.\n",
    "\n",
    "Par contre, le second problème est convexe mais non 3-différentiable. Pour cela on a utilisé le gradient proxial étant donnée une fonction dont le proximal a été vu en cours et est simple à calculer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les deux régularisations donnent des solutions proches mais la rapidité de la méthode de newton est nettement meilleure bien que le nombre d'itérations soit grand l'algorithme converge rapidement (temporellement)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
